{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/anaconda3/envs/OT/lib/python3.8/site-packages/scipy/optimize/_hessian_update_strategy.py:182: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  warn('delta_grad == 0.0. Check if the approximated '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import differential_evolution, LinearConstraint, Bounds\n",
    "from scipy.special import softmax\n",
    "import modeling\n",
    "\n",
    "def my_objective_func(params, D):\n",
    "\t# unpack the parameters\n",
    "\t[alpha_2, beta, beta_meta, concentration_2, epsilon, prior_1, prior_2] = params\n",
    "\tif alpha_2 < 0 or epsilon < 0 or prior_1 < 0 or prior_2 < 0:\n",
    "\t\tprint(f\"alpha_2: {alpha_2}, beta: {beta}, beta_meta: {beta_meta}, concentration_2: {concentration_2}, epsilon: {epsilon}, prior_1: {prior_1}, prior_2: {prior_2}\")\n",
    "\tbeta_2 = beta\n",
    "\tbeta_policies = 5\n",
    "\tbeta_meta = 10**beta_meta\n",
    "\tconcentration_2 = 10**concentration_2\n",
    "\t\n",
    "\t# initialize variables\n",
    "\tllh = 0\n",
    "\tnum_block = 12\n",
    "\ts_2 = a_2 = -1\n",
    "\tblock = -1\n",
    "\n",
    "\tnTS_2 = 2 # number of task-sets in stage2\n",
    "\tTS_2s = np.ones((nTS_2,2,4)) / 4 # Q-values for each TS in stage2\n",
    "\tnC_2 = 2 * num_block # number of contexts in stage2\n",
    "\tPTS_2 = np.zeros((nTS_2,nC_2)) # probability of choosing each TS in each context\n",
    "\tPTS_2[0,0::2] = 1\n",
    "\tPTS_2[1,1::2] = 1\n",
    "\tencounter_matrix_2 = np.zeros(nC_2) # whether a context has been encountered\n",
    "\tencounter_matrix_2[:nTS_2] = 1\n",
    "\tp_policies = np.array([prior_1, prior_2, 1-prior_1-prior_2]) # probability of sampling each policy\n",
    "\tp_policies_softmax = softmax(beta_policies * p_policies) # softmax transform of the policy probabilities\n",
    "\n",
    "\tfor t in range(D.shape[0]):\t# loop over all trials\n",
    "\t\tstage = int(D[t,1])\n",
    "\n",
    "\t\tif int(D[t,5]) == 1: # new block\n",
    "\t\t\tblock += 1\n",
    "\n",
    "\t\tif stage == 1: # skip stage1\n",
    "\t\t\ts_1 = int(D[t, 2])\n",
    "\t\t\tactions_tried = set()\n",
    "\t\telif stage == 2: # stage2\n",
    "\t\t\t# get stim, action, reward info\n",
    "\t\t\ts_2 = int(D[t, 2])\n",
    "\t\t\ta_2 = int(D[t, 3]) - 4\n",
    "\t\t\tr_2 = int(D[t, 4])\n",
    "\n",
    "\t\t\t# get the context and state, determined by model structure\n",
    "\t\t\tcue = s_2\n",
    "\t\t\tstate = s_1\n",
    "\t\t\tc_2 = block * 2 + cue # The context of stage2\n",
    "\t\t\tc_2_alt = block * 2 + (1 - cue) # The alternative context of stage2\n",
    "\t\t\t# update the PTS matrix with newly encountered context, if any\n",
    "\t\t\tfor this_c_2 in sorted([c_2, c_2_alt]):\n",
    "\t\t\t\tif encounter_matrix_2[this_c_2] == 0:\n",
    "\t\t\t\t\tif this_c_2 > 0:\n",
    "\t\t\t\t\t\tspecs = PTS_2.shape\n",
    "\t\t\t\t\t\tPTS_2[:,this_c_2] = np.sum(this_c_2[:,:(this_c_2//2)*2], axis=1)\n",
    "\t\t\t\t\t\tif np.sum(PTS_2[:,c]) > 0:\n",
    "\t\t\t\t\t\t\tPTS_2[:,this_c_2] /= np.sum(this_c_2[:,this_c_2])\n",
    "\t\t\t\t\t\t\tnew_PTS = np.zeros((specs[0]+1,specs[1]))\n",
    "\t\t\t\t\t\t\tnew_PTS[:-1,:] = PTS_2 \n",
    "\t\t\t\t\t\t\tnew_PTS[-1,this_c_2] = concentration_2 # create new task set\n",
    "\t\t\t\t\t\t\tnew_PTS[:,this_c_2] /= np.sum(new_PTS[:,this_c_2]) # normalize the probability distribution over the set of TS\n",
    "\t\t\t\t\t\tPTS_2 = new_PTS\n",
    "\t\t\t\t\t\tTS_2s = np.vstack((TS_2s, [np.ones((2,4)) / 4])) # initialize Q-values for new TS creation\n",
    "\t\t\t\t\t\tnTS_2 += 1\n",
    "\t\t\t\t\tencounter_matrix_2[this_c_2] = 1\n",
    "\n",
    "\t\t\t# update the biases matrix for context-pairing\n",
    "\t\t\tbiases = np.zeros((nTS_2, nTS_2))\n",
    "\t\t\tfor i in range(block):\n",
    "\t\t\t\tthis_TS_1 = np.argmax(PTS_2[:-2,i*2])\n",
    "\t\t\t\tthis_TS_2 = np.argmax(PTS_2[:-2,i*2+1])\n",
    "\t\t\t\tbiases[this_TS_1, this_TS_2] += 1\n",
    "\t\t\t\tbiases[this_TS_2, this_TS_1] += 1\n",
    "\n",
    "\t\t\t# incorporate the biases into task-set probabilities\n",
    "\t\t\tTS_2_alt = np.argmax(PTS_2[:,c_2_alt])\n",
    "\t\t\tif block > 0:\n",
    "\t\t\t\tbias = biases[TS_2_alt].copy()\n",
    "\t\t\t\tb = np.max(PTS_2[:,c_2_alt])\n",
    "\t\t\t\tif np.sum(bias) > 0 and np.max(PTS_2[:,c_2]) < 0.5:\n",
    "\t\t\t\t\tbias /= np.sum(bias)\n",
    "\t\t\t\t\tPTS_2[:,c_2] = PTS_2[:,c_2] * (1 - b) + bias * b\n",
    "\n",
    "\t\t\t# sample a task-set based on the TS probabilities given the context\n",
    "\t\t\tQ_full = TS_2s[:, state].copy()\n",
    "\t\t\tif len(actions_tried) > 0:\n",
    "\t\t\t\tQ_full[:,list(actions_tried)] = -1e20\n",
    "\t\t\tpchoice_2_full = softmax(beta_2 * Q_full, axis=-1)\n",
    "\t\t\tpchoice_2_full = np.sum(pchoice_2_full[:,a_2-1] * PTS_2[:,c_2]) * (1-epsilon) + epsilon / 4\n",
    "\n",
    "\t\t\t# compute the choice policy\n",
    "\t\t\tQ_compress_1 = np.mean(TS_2s, axis=(1)) # compressed policy over stage1\n",
    "\t\t\tQ_compress_2 = (TS_2s + np.sum(TS_2s * PTS_2[:,c_2_alt].reshape(-1,1,1),axis=0))[:,state] / 2 # compressed policy over stage2\n",
    "            \n",
    "            # avoid choosing the same action in the same stage of the same trial\n",
    "\t\t\tif len(actions_tried) > 0:\n",
    "\t\t\t\tQ_compress_1[:,list(actions_tried)] = -1e20\n",
    "\t\t\t\tQ_compress_2[:,list(actions_tried)] = -1e20\n",
    "            # compute the choice policies for compressed policies\n",
    "\t\t\tpchoice_2_compress_1 = softmax(beta_2 * Q_compress_1, axis=-1)\n",
    "\t\t\tpchoice_2_compress_1 = np.sum(pchoice_2_compress_1[:,a_2-1] * PTS_2[:,c_2]) * (1-epsilon) + epsilon / 4\n",
    "\t\t\tpchoice_2_compress_2 = softmax(beta_2 * Q_compress_2, axis=-1) \n",
    "\t\t\tpchoice_2_compress_2 = np.sum(pchoice_2_compress_2[:,a_2-1] * PTS_2[:,c_2]) * (1-epsilon) + epsilon / 4\n",
    "            # take a weighted sum of the two compressed policies and the fully hierarchical policy\n",
    "\t\t\tpchoice_2 = p_policies_softmax[0] * pchoice_2_compress_1 \\\n",
    "\t\t\t            + p_policies_softmax[1] * pchoice_2_compress_2 \\\n",
    "\t\t\t            + p_policies_softmax[2] * pchoice_2_full\n",
    "\n",
    "\t\t\t# compute the negative log likelihood of the choice based on the choice policy\n",
    "\t\t\tif np.isnan(pchoice_2) or pchoice_2 <= 0:\n",
    "\t\t\t\tprint(f\"pchoice_2: {pchoice_2}\")\n",
    "\t\t\t\tprint(f\"prior_1: {prior_1}, prior_2: {prior_2}, epsilon: {epsilon}\")\n",
    "\t\t\t\tprint(f\"p_policies_softmax: {p_policies_softmax}\")\n",
    "\t\t\t\tprint(f\"pchoice_2_compress_1: {pchoice_2_compress_1}\")\n",
    "\t\t\t\tprint(f\"pchoice_2_compress_2: {pchoice_2_compress_2}\")\n",
    "\t\t\t\tprint(f\"pchoice_2_full: {pchoice_2_full}\")\n",
    "\t\t\tllh += np.log(pchoice_2)\n",
    "\t\t\tcorrect_2 = r_2\n",
    "\n",
    "\t\t\t# update the task-set probabilities based on the choice and the reward using Bayes Rule\n",
    "\t\t\tPTS_2[:,c_2] *= (1 - correct_2 - (-1)**correct_2 * TS_2s[:, state, a_2-1])\n",
    "\t\t\tPTS_2[:,c_2] += 1e-6\n",
    "\t\t\tPTS_2[:,c_2] /= np.sum(PTS_2[:,c_2])\n",
    "\n",
    "\t\t\t# update the Q-values of the task-sets based on the reward prediction error\n",
    "\t\t\tRPE = (r_2 - TS_2s[:,state,a_2-1]) * PTS_2[:,c_2]\n",
    "\t\t\tTS_2s[:,state,a_2-1] += alpha_2 * RPE\n",
    "\n",
    "\t\t\t# update the policy probabilities using Bayes Rule\n",
    "\t\t\tlikelihoods = np.array([pchoice_2_compress_1, pchoice_2_compress_2, pchoice_2_full])\n",
    "\t\t\tlikelihoods = softmax(beta_meta * likelihoods)\n",
    "\t\t\tp_policies *= (1 - correct_2 - (-1)**correct_2 * likelihoods)\n",
    "\t\t\tif np.min(p_policies) < 1e-6:\n",
    "\t\t\t\tp_policies += 1e-6\n",
    "\t\t\tp_policies /= np.sum(p_policies)\n",
    "\t\t\tp_policies_softmax = softmax(beta_policies * p_policies)\n",
    "\n",
    "\t\t\tactions_tried.add(a_2-1)\n",
    "\n",
    "\treturn -llh\n",
    "\n",
    "# np.random.seed(2848769375)\n",
    "X = np.load('D.npy')\n",
    "constraints = LinearConstraint([0,0,0,0,0,1,1], lb=2e-6, ub=1-1e-6, keep_feasible=True)\n",
    "bounds = Bounds(lb=[1e-6, 1e-6, -6, -2, 1e-6, 1e-6, 1e-6], ub=[1, 20, 0.5, 0.7, 1, 1, 1])\n",
    "result = differential_evolution(func=modeling.abstraction_model_nllh, bounds=bounds, constraints=constraints, args=(X, 'backward', True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current random seed: 2848769375\n"
     ]
    }
   ],
   "source": [
    "current_seed = np.random.get_state()[1][0]\n",
    "print(f\"Current random seed: {current_seed}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
